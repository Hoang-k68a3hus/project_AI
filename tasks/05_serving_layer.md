# Task 05: Serving Layer

## Má»¥c TiÃªu

XÃ¢y dá»±ng service layer Ä‘á»ƒ serve recommendations trong production, bao gá»“m model loading, recommendation generation, **user segmentation routing**, cold-start handling, filtering logic, vÃ  optional API endpoints. Service pháº£i Ä‘áº£m báº£o latency tháº¥p, reliability cao, vÃ  dá»… dÃ ng integration.

## ðŸ”„ Updated Serving Strategy (November 2025)

### Context: High Sparsity Data (~1.23 interactions/user, Updated â‰¥2 Threshold)
- **Trainable users** (â‰¥2 interactions): ~26,000 users (~8.6% of total) â†’ Serve with CF (ALS/BPR) + reranking
- **Cold-start users** (1 interaction or new): ~274,000 users (~91.4%) â†’ Serve with content-based + popularity
- **Routing decision**: Load `user_metadata.pkl` to check `is_trainable_user` flag
- **Key insight**: 90%+ traffic will use content-based; CF is for the minority with â‰¥2 interactions

### Serving Flow by User Type:

```
User Request
    â†“
Check user_metadata
    â†“
â”œâ”€ Trainable User? (â‰¥2 interactions + â‰¥1 positive)
â”‚  â”œâ”€ CF Recommender (ALS/BPR)
â”‚  â”œâ”€ Generate Top-K candidates
â”‚  â”œâ”€ Hybrid Reranking (CF + Content + Popularity)
â”‚  â””â”€ Return personalized results (~8.6% of traffic)
â”‚
â””â”€ Cold-Start User? (1 interaction or new user)
   â”œâ”€ Skip CF (no reliable user embedding)
   â”œâ”€ Item-Item Similarity (PhoBERT)
   â”‚  â””â”€ Find similar products to user's purchase history
   â”œâ”€ Mix with Popularity (Top sellers)
   â””â”€ Return content-based + popular results (~91.4% of traffic)
```

**Benefits**:
- Don't waste CF computation on users with insufficient data
- Content-based provides better recommendations for sparse users than weak CF embeddings
- Clear separation of concerns for monitoring and A/B testing
- **Traffic optimization**: ~91.4% traffic uses fast content-based path; only ~8.6% uses CF

## Architecture Overview

```
service/
â”œâ”€â”€ recommender/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py           # Load models tá»« registry
â”‚   â”œâ”€â”€ recommender.py      # Core recommendation logic
â”‚   â”œâ”€â”€ rerank.py          # Hybrid reranking (optional)
â”‚   â”œâ”€â”€ filters.py         # Attribute filtering
â”‚   â””â”€â”€ fallback.py        # Cold-start handling
â”œâ”€â”€ api.py                 # REST API (FastAPI)
â””â”€â”€ config/
    â””â”€â”€ serving_config.yaml
```

## Component 1: Model Loader

### Module: `service/recommender/loader.py`

#### Class: `CFModelLoader`

##### Purpose
Singleton class quáº£n lÃ½ model loading, caching, vÃ  hot-reload khi registry updates.

##### Attributes
```python
class CFModelLoader:
    def __init__(self, registry_path='artifacts/cf/registry.json'):
        self.registry_path = registry_path
        self.current_model = None  # Cached model
        self.current_model_id = None
        self.mappings = None  # User/item mappings
        self.item_metadata = None  # Product info
```

##### Method 1: `load_model(model_id=None)`
```python
def load_model(self, model_id=None):
    """
    Load CF model tá»« registry.
    
    Args:
        model_id: Optional model ID. Náº¿u None â†’ load current_best
    
    Returns:
        dict: {
            'model_id': str,
            'model_type': 'als' | 'bpr',
            'U': np.array (num_users, factors),
            'V': np.array (num_items, factors),
            'params': dict,
            'metadata': dict
        }
    
    Raises:
        FileNotFoundError: Model artifacts khÃ´ng tá»“n táº¡i
        ValueError: Invalid model_id
    """
    # Load registry
    registry = load_registry_json(self.registry_path)
    
    # Determine model to load
    if model_id is None:
        model_id = registry['current_best']['model_id']
    
    # Get model info
    model_info = registry['models'][model_id]
    model_path = model_info['path']
    
    # Load embeddings
    U = np.load(f"{model_path}/{model_info['model_type']}_U.npy")
    V = np.load(f"{model_path}/{model_info['model_type']}_V.npy")
    
    # Load params
    # (Ä‘áº£m báº£o module Ä‘Ã£ import os)
    param_pattern = os.path.join(model_path, f"{model_info['model_type']}_params.json")
    with open(param_pattern) as f:
        params = json.load(f)
    
    # Cache
    self.current_model = {
        'model_id': model_id,
        'model_type': model_info['model_type'],
        'U': U,
        'V': V,
        'params': params,
        'metadata': model_info
    }
    self.current_model_id = model_id
    
    return self.current_model
```

##### Method 2: `load_mappings(data_version=None)`
```python
def load_mappings(self, data_version=None):
    """
    Load user/item ID mappings.
    
    Args:
        data_version: Optional hash. Náº¿u None â†’ load latest
    
    Returns:
        dict: {
            'user_to_idx': {user_id: u_idx},
            'idx_to_user': {u_idx: user_id},
            'item_to_idx': {product_id: i_idx},
            'idx_to_item': {i_idx: product_id},
            'metadata': {...}
        }
    """
    path = 'data/processed/user_item_mappings.json'
    
    with open(path) as f:
        mappings = json.load(f)
    
    # Validate data version náº¿u cÃ³
    if data_version and mappings['metadata']['data_hash'] != data_version:
        warnings.warn(f"Data version mismatch: {data_version} vs {mappings['metadata']['data_hash']}")
    
    self.mappings = mappings
    return mappings
```

##### Method 3: `load_item_metadata()`
```python
def load_item_metadata(self):
    """
    Load product metadata cho enrichment.
    
    Returns:
        pd.DataFrame: Products vá»›i columns [product_id, product_name, brand, ...]
    """
    products = pd.read_csv('data/published_data/data_product.csv', encoding='utf-8')
    attributes = pd.read_csv('data/published_data/data_product_attribute.csv', encoding='utf-8')
    
    # Merge
    metadata = products.merge(attributes, on='product_id', how='left')
    
    # Cache
    self.item_metadata = metadata
    return metadata
```

##### Method 4: `reload_if_updated()`
```python
def reload_if_updated(self):
    """
    Check registry for updates vÃ  reload náº¿u current_best changed.
    
    Returns:
        bool: True náº¿u reloaded, False otherwise
    """
    registry = load_registry_json(self.registry_path)
    new_best_id = registry['current_best']['model_id']
    
    if new_best_id != self.current_model_id:
        logger.info(f"Registry updated: {self.current_model_id} â†’ {new_best_id}")
        self.load_model(new_best_id)
        return True
    
    return False
```

## Component 2: Core Recommender

### Module: `service/recommender/recommender.py`

#### Class: `CFRecommender`

##### Purpose
Main recommendation engine vá»›i scoring, filtering, ranking logic.

##### Initialization
```python
class CFRecommender:
    def __init__(self, model_loader: CFModelLoader):
        self.loader = model_loader
        
        # Load artifacts
        self.model = model_loader.load_model()
        self.mappings = model_loader.load_mappings(
            data_version=self.model['metadata'].get('data_version')
        )
        self.item_metadata = model_loader.load_item_metadata()
        
        # Precompute
        self.U = self.model['U']
        self.V = self.model['V']
        self.num_items = self.V.shape[0]
        self.user_history_cache = self._load_user_histories()
```

##### Helper: `_load_user_histories()`
```python
def _load_user_histories(self):
    """
    Preload user â†’ product interactions once Ä‘á»ƒ phá»¥c vá»¥ low-latency.
    """
    interactions = pd.read_parquet(
        'data/processed/interactions.parquet',
        columns=['user_id', 'product_id']
    )
    history = (
        interactions.groupby('user_id')['product_id']
        .apply(lambda s: set(s.tolist()))
        .to_dict()
    )
    return history
```

##### Method 1: `recommend(user_id, topk=10, exclude_seen=True, filter_params=None)`
```python
def recommend(self, user_id, topk=10, exclude_seen=True, filter_params=None):
    """
    Generate top-K recommendations cho user.
    
    Args:
        user_id: Original user ID (int)
        topk: Number of recommendations (default 10)
        exclude_seen: Náº¿u True, loáº¡i bá» items user Ä‘Ã£ tÆ°Æ¡ng tÃ¡c
        filter_params: Dict vá»›i attribute filters (e.g., {'brand': 'Innisfree'})
    
    Returns:
        list of dict: [
            {
                'product_id': int,
                'score': float,
                'product_name': str,
                'brand': str,
                'price': float,
                ...
            },
            ...
        ]
    
    Raises:
        KeyError: User ID khÃ´ng tá»“n táº¡i (cold-start)
    """
    # Check user exists
    if user_id not in self.mappings['user_to_idx']:
        # Cold-start fallback
        return self._fallback_recommendations(topk, filter_params)
    
    # Map user_id â†’ u_idx
    u_idx = self.mappings['user_to_idx'][user_id]
    
    # Compute scores
    scores = self.U[u_idx] @ self.V.T  # Shape: (num_items,)
    
    # Exclude seen items
    if exclude_seen:
        seen_items = self._get_user_history(user_id)
        seen_indices = [self.mappings['item_to_idx'][pid] for pid in seen_items if pid in self.mappings['item_to_idx']]
        scores[seen_indices] = -np.inf
    
    # Apply attribute filters
    if filter_params:
        valid_indices = self._apply_filters(filter_params)
        mask = np.ones(self.num_items, dtype=bool)
        mask[valid_indices] = False
        scores[mask] = -np.inf
    
    # Top-K
    top_k_indices = np.argsort(scores)[::-1][:topk]
    
    # Map i_idx â†’ product_id
    product_ids = [self.mappings['idx_to_item'][str(i)] for i in top_k_indices]
    
    # Enrich vá»›i metadata
    recommendations = []
    for i, pid in enumerate(product_ids):
        product_info = self.item_metadata[self.item_metadata['product_id'] == pid].iloc[0].to_dict()
        recommendations.append({
            'product_id': pid,
            'score': float(scores[top_k_indices[i]]),
            'rank': i + 1,
            **product_info  # product_name, brand, price, ...
        })
    
    return recommendations
```

##### Method 2: `_get_user_history(user_id)`
```python
def _get_user_history(self, user_id):
    """
    Retrieve items user Ä‘Ã£ interact.
    
    Args:
        user_id: Original user ID
    
    Returns:
        set: Set of product_ids
    """
    cached = self.user_history_cache.get(user_id)
    return set(cached) if cached else set()
```

##### Method 3: `_apply_filters(filter_params)`
```python
def _apply_filters(self, filter_params):
    """
    Apply attribute filters.
    
    Args:
        filter_params: Dict nhÆ° {'brand': 'Innisfree', 'skin_type': 'oily'}
    
    Returns:
        np.array: Indices of valid items
    """
    mask = pd.Series([True] * len(self.item_metadata))
    
    for key, value in filter_params.items():
        if key in self.item_metadata.columns:
            mask &= self.item_metadata[key] == value
    
    valid_product_ids = self.item_metadata[mask]['product_id'].values
    valid_indices = [self.mappings['item_to_idx'][str(pid)] for pid in valid_product_ids]
    
    return np.array(valid_indices)
```

##### Method 4: `batch_recommend(user_ids, topk=10, exclude_seen=True)`
```python
def batch_recommend(self, user_ids, topk=10, exclude_seen=True):
    """
    Batch recommendation cho nhiá»u users (efficient).
    
    Args:
        user_ids: List of user IDs
        topk: Number of recommendations per user
        exclude_seen: Filter seen items
    
    Returns:
        dict: {user_id: [recommendations]}
    """
    results = {}
    
    # Separate cold-start users
    known_users = [uid for uid in user_ids if uid in self.mappings['user_to_idx']]
    cold_users = [uid for uid in user_ids if uid not in self.mappings['user_to_idx']]
    
    # Batch scoring cho known users
    if known_users:
        u_indices = [self.mappings['user_to_idx'][uid] for uid in known_users]
        scores_batch = self.U[u_indices] @ self.V.T  # (len(known_users), num_items)
        
        for i, uid in enumerate(known_users):
            scores = scores_batch[i]
            
            if exclude_seen:
                seen = self._get_user_history(uid)
                seen_indices = [self.mappings['item_to_idx'][str(pid)] for pid in seen]
                scores[seen_indices] = -np.inf
            
            top_k_indices = np.argsort(scores)[::-1][:topk]
            product_ids = [self.mappings['idx_to_item'][str(i)] for i in top_k_indices]
            
            # Enrich (simplified)
            results[uid] = [{'product_id': pid, 'score': float(scores[i])} for pid, i in zip(product_ids, top_k_indices)]
    
    # Fallback cho cold-start
    for uid in cold_users:
        results[uid] = self._fallback_recommendations(topk)
    
    return results
```

## Component 3: Cold-Start Fallback (UPDATED)

### Module: `service/recommender/fallback.py`

#### Strategy Overview
For users with 1-2 interactions (cold-start), skip CF and use **Item-Item Similarity + Popularity**.

#### Function 1: `_fallback_item_similarity(user_history, topk, filter_params=None)`

##### Purpose: Content-Based Recommendations
```python
def _fallback_item_similarity(self, user_history, topk=10, filter_params=None):
    """
    Fallback strategy: Find similar items to user's purchase history using PhoBERT.
    
    Args:
        user_history: List of product_ids user has interacted with
        topk: Number of recommendations
        filter_params: Optional attribute filters
    
    Returns:
        list of dict: Content-similar products ranked by relevance
    """
    if not user_history:
        # Truly new user â†’ pure popularity
        return self._fallback_popularity(topk, filter_params)
    
    # Load PhoBERT embeddings
    if not hasattr(self, '_phobert_loader'):
        from service.recommender.phobert_loader import PhoBERTEmbeddingLoader
        self._phobert_loader = PhoBERTEmbeddingLoader()
    
    # Get embeddings for user history
    history_embeddings = []
    for pid in user_history:
        emb = self._phobert_loader.get_embedding(pid)
        if emb is not None:
            history_embeddings.append(emb)
    
    if not history_embeddings:
        return self._fallback_popularity(topk, filter_params)
    
    # Compute user profile (mean of history embeddings)
    user_profile = np.mean(history_embeddings, axis=0)
    
    # Compute similarity to all items
    all_item_ids = self.item_metadata['product_id'].tolist()
    similarities = {}
    
    for pid in all_item_ids:
        if pid in user_history:
            continue  # Skip already purchased
        
        item_emb = self._phobert_loader.get_embedding(pid)
        if item_emb is not None:
            sim = cosine_similarity(user_profile, item_emb)
            similarities[pid] = sim
    
    # Apply filters
    if filter_params:
        filtered_metadata = self.item_metadata.copy()
        for key, value in filter_params.items():
            if key in filtered_metadata.columns:
                filtered_metadata = filtered_metadata[filtered_metadata[key] == value]
        valid_pids = set(filtered_metadata['product_id'].tolist())
        similarities = {pid: sim for pid, sim in similarities.items() if pid in valid_pids}
    
    # Rank by similarity
    top_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:topk]
    
    # Format output
    recommendations = []
    for rank, (pid, sim_score) in enumerate(top_similar, 1):
        product_info = self.item_metadata[self.item_metadata['product_id'] == pid].iloc[0]
        recommendations.append({
            'product_id': pid,
            'score': float(sim_score),
            'rank': rank,
            'fallback': True,
            'fallback_method': 'item_similarity',
            **product_info.to_dict()
        })
    
    return recommendations


def cosine_similarity(vec1, vec2):
    """Compute cosine similarity between two vectors."""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

#### Function 2: `_fallback_popularity(topk, filter_params=None)`

##### Purpose: Pure Popularity for Truly New Users
```python
def _fallback_popularity(self, topk=10, filter_params=None):
    """
    Fallback tá»›i popularity ranking cho truly new users (no history).
    
    Args:
        topk: Number of recommendations
        filter_params: Optional attribute filters
    
    Returns:
        list of dict: Popular products
    """
    # Sort by popularity_score (log-transformed from Task 01)
    popular = self.item_metadata.sort_values('popularity_score', ascending=False)
    
    # Apply filters náº¿u cÃ³
    if filter_params:
        for key, value in filter_params.items():
            if key in popular.columns:
                # Handle standardized lists (e.g., skin_type_standardized)
                if isinstance(value, list):
                    popular = popular[popular[key].apply(lambda x: any(v in x for v in value))]
                else:
                    popular = popular[popular[key] == value]
    
    # Top-K
    top_popular = popular.head(topk)
    
    # Format output
    recommendations = []
    for i, row in top_popular.iterrows():
        recommendations.append({
            'product_id': row['product_id'],
            'score': row['popularity_score'],  # Log-transformed popularity
            'rank': len(recommendations) + 1,
            'fallback': True,
            'fallback_method': 'popularity',
            **row.to_dict()
        })
    
    return recommendations
```

#### Function 3: `_hybrid_fallback(user_history, topk, content_weight=0.7, popularity_weight=0.3)`

##### Purpose: Mix Content Similarity + Popularity
```python
def _hybrid_fallback(self, user_history, topk=10, content_weight=0.7, popularity_weight=0.3):
    """
    Hybrid fallback: Combine item similarity with popularity.
    
    Args:
        user_history: User's purchase history
        topk: Number of recommendations
        content_weight: Weight for content similarity (default 0.7)
        popularity_weight: Weight for popularity (default 0.3)
    
    Returns:
        list of dict: Hybrid recommendations
    """
    # Get content-based candidates (2x topk for diversity)
    content_recs = self._fallback_item_similarity(user_history, topk=topk*2)
    
    # Create score dict
    scores = {}
    for rec in content_recs:
        pid = rec['product_id']
        # Normalize content score to [0,1]
        content_score = rec['score']
        
        # Get popularity score (already normalized in enriched metadata)
        product_info = self.item_metadata[self.item_metadata['product_id'] == pid].iloc[0]
        pop_score = product_info['popularity_score']
        
        # Weighted combination
        final_score = content_weight * content_score + popularity_weight * pop_score
        scores[pid] = {
            'final_score': final_score,
            'content_score': content_score,
            'popularity_score': pop_score,
            'product_info': product_info
        }
    
    # Rank by final score
    top_items = sorted(scores.items(), key=lambda x: x[1]['final_score'], reverse=True)[:topk]
    
    # Format output
    recommendations = []
    for rank, (pid, data) in enumerate(top_items, 1):
        recommendations.append({
            'product_id': pid,
            'score': data['final_score'],
            'rank': rank,
            'fallback': True,
            'fallback_method': 'hybrid',
            'content_score': data['content_score'],
            'popularity_score': data['popularity_score'],
            **data['product_info'].to_dict()
        })
    
    return recommendations
```

## Component 4: Hybrid Reranking (Optional)

### Module: `service/recommender/rerank.py`

#### Function: `rerank_with_signals(recommendations, user_id, weights=None)`

##### Purpose
Combine CF scores vá»›i additional signals (popularity, quality, content similarity).

##### Formula
```
final_score = Î± * CF_score + Î² * popularity + Î³ * quality + Î´ * content_similarity
```

##### Implementation
```python
def rerank_with_signals(recommendations, user_id, weights=None):
    """
    Rerank recommendations báº±ng weighted combination of signals.
    
    Args:
        recommendations: List tá»« CFRecommender.recommend()
        user_id: For personalized content similarity (optional)
        weights: Dict {'cf': 0.5, 'popularity': 0.2, 'quality': 0.2, 'content': 0.1}
    
    Returns:
        list: Reranked recommendations
    """
    if weights is None:
        weights = {'cf': 0.6, 'popularity': 0.2, 'quality': 0.2, 'content': 0.0}
    
    # Normalize signals
    cf_scores = normalize([r['score'] for r in recommendations])
    popularity_scores = normalize([r.get('num_sold_time', 0) for r in recommendations])
    quality_scores = normalize([r.get('avg_star', 3.0) for r in recommendations])
    
    # Content similarity (náº¿u cÃ³ PhoBERT embeddings)
    if weights['content'] > 0:
        content_scores = compute_content_similarity(user_id, [r['product_id'] for r in recommendations])
    else:
        content_scores = [0] * len(recommendations)
    
    # Combine
    for i, rec in enumerate(recommendations):
        rec['final_score'] = (
            weights['cf'] * cf_scores[i] +
            weights['popularity'] * popularity_scores[i] +
            weights['quality'] * quality_scores[i] +
            weights['content'] * content_scores[i]
        )
    
    # Re-sort
    recommendations.sort(key=lambda x: x['final_score'], reverse=True)
    
    # Update ranks
    for i, rec in enumerate(recommendations):
        rec['rank'] = i + 1
    
    return recommendations
```

## Component 5: API Layer (FastAPI)

### Module: `service/api.py`

#### Endpoints

##### 1. Health Check
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="CF Recommendation Service")

@app.get("/health")
def health_check():
    """Check service status."""
    return {
        "status": "healthy",
        "model_id": recommender.model['model_id'],
        "model_type": recommender.model['model_type']
    }
```

##### 2. Single User Recommendation
```python
class RecommendRequest(BaseModel):
    user_id: int
    topk: int = 10
    exclude_seen: bool = True
    filter_params: dict = None

@app.post("/recommend")
def recommend(request: RecommendRequest):
    """
    Get recommendations cho single user.
    
    Example:
        POST /recommend
        {
            "user_id": 12345,
            "topk": 10,
            "exclude_seen": true,
            "filter_params": {"brand": "Innisfree"}
        }
    """
    try:
        recommendations = recommender.recommend(
            user_id=request.user_id,
            topk=request.topk,
            exclude_seen=request.exclude_seen,
            filter_params=request.filter_params
        )
        
        return {
            "user_id": request.user_id,
            "recommendations": recommendations,
            "count": len(recommendations),
            "fallback": recommendations[0].get('fallback', False) if recommendations else False
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

##### 3. Batch Recommendation
```python
class BatchRequest(BaseModel):
    user_ids: list[int]
    topk: int = 10
    exclude_seen: bool = True

@app.post("/batch_recommend")
def batch_recommend(request: BatchRequest):
    """
    Batch recommendations cho multiple users.
    """
    results = recommender.batch_recommend(
        user_ids=request.user_ids,
        topk=request.topk,
        exclude_seen=request.exclude_seen
    )
    
    return {
        "results": results,
        "num_users": len(request.user_ids)
    }
```

##### 4. Model Reload
```python
@app.post("/reload_model")
def reload_model():
    """
    Reload model tá»« registry (hot-reload).
    """
    updated = loader.reload_if_updated()
    
    if updated:
        # Reinitialize recommender vá»›i new model
        global recommender
        recommender = CFRecommender(loader)
        
        return {
            "status": "reloaded",
            "new_model_id": recommender.model['model_id']
        }
    else:
        return {
            "status": "no_update",
            "current_model_id": recommender.model['model_id']
        }
```

## Configuration

### File: `service/config/serving_config.yaml`
```yaml
model:
  registry_path: "artifacts/cf/registry.json"
  auto_reload: true  # Periodically check registry
  reload_interval_seconds: 300  # 5 minutes

serving:
  default_topk: 10
  max_topk: 100
  exclude_seen_default: true
  
fallback:
  strategy: "popularity"  # or "phobert"
  popularity_metric: "num_sold_time"  # or "avg_star"

reranking:
  enabled: false
  weights:
    cf: 0.6
    popularity: 0.2
    quality: 0.2
    content: 0.0

api:
  host: "0.0.0.0"
  port: 8000
  workers: 4  # Uvicorn workers
  log_level: "info"
```

## Logging & Monitoring

### Request Logging
```python
import logging
from datetime import datetime

logger = logging.getLogger("cf_service")

@app.post("/recommend")
def recommend(request: RecommendRequest):
    start_time = datetime.now()
    
    try:
        recommendations = recommender.recommend(...)
        
        latency = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"user_id={request.user_id}, topk={request.topk}, latency={latency:.3f}s, fallback={recommendations[0].get('fallback', False)}")
        
        return {...}
    except Exception as e:
        logger.error(f"user_id={request.user_id}, error={str(e)}")
        raise
```

### Metrics to Track
- **Latency**: p50, p95, p99 response time
- **Throughput**: Requests per second
- **Fallback rate**: % requests sá»­ dá»¥ng fallback
- **Error rate**: % failed requests
- **Cache hit rate**: Náº¿u cÃ³ caching layer

## Performance Optimization

### 1. Precompute Item-Item Similarity (Optional)
```python
# For "similar items" recommendations
self.V_dot_V = self.V @ self.V.T  # (num_items, num_items)

def similar_items(self, product_id, topk=10):
    i_idx = self.mappings['item_to_idx'][str(product_id)]
    scores = self.V_dot_V[i_idx]
    top_k = np.argsort(scores)[::-1][1:topk+1]  # Exclude self
    return [self.mappings['idx_to_item'][str(i)] for i in top_k]
```

### 2. Caching User History
```python
from functools import lru_cache

@lru_cache(maxsize=10000)
def _get_user_history_cached(self, user_id):
    return self._get_user_history(user_id)
```

### 3. Batch Inference
- Process multiple users simultaneously â†’ amortize overhead
- Use NumPy vectorization

## Deployment

### Docker Container
```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY service/ service/
COPY artifacts/ artifacts/
COPY data/processed/ data/processed/

CMD ["uvicorn", "service.api:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### Run Locally
```bash
# Install dependencies
pip install fastapi uvicorn

# Start service
uvicorn service.api:app --reload --port 8000
```

### Test Endpoint
```bash
curl -X POST http://localhost:8000/recommend \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 12345,
    "topk": 10,
    "exclude_seen": true
  }'
```

## Component 6: BERT Embeddings Integration

### Module: `service/recommender/phobert_loader.py`

#### Class: `PhoBERTEmbeddingLoader`

```python
import numpy as np
import torch
from functools import lru_cache

class PhoBERTEmbeddingLoader:
    """
    Load vÃ  cache BERT embeddings cho serving.
    """
    
    def __init__(self, embeddings_path='data/processed/content_based_embeddings/product_embeddings.pt'):
        self.embeddings_path = embeddings_path
        self.embeddings = None  # (num_products, 768)
        self.product_id_to_idx = {}
        self.idx_to_product_id = {}
        self._load_embeddings()
    
    def _load_embeddings(self):
        """Load BERT embeddings vÃ  create mappings."""
        bert_data = torch.load(self.embeddings_path, map_location='cpu')
        
        self.embeddings = bert_data['embeddings'].numpy()  # (N, 768)
        product_ids = bert_data['product_ids']
        
        # Create mappings
        for idx, pid in enumerate(product_ids):
            self.product_id_to_idx[int(pid)] = idx
            self.idx_to_product_id[idx] = int(pid)
        
        # Normalize for fast cosine similarity
        norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)
        self.embeddings_norm = self.embeddings / norms
        
        print(f"Loaded {len(self.product_id_to_idx)} BERT embeddings (dim={self.embeddings.shape[1]})")
    
    def get_embedding(self, product_id):
        """Get embedding for single product."""
        idx = self.product_id_to_idx.get(product_id)
        if idx is not None:
            return self.embeddings[idx]
        return None
    
    def compute_user_profile(self, user_history_items, strategy='weighted_mean'):
        """
        Compute user profile embedding tá»« interaction history.
        
        Args:
            user_history_items: List[(product_id, weight)] or List[product_id]
            strategy: 'mean', 'weighted_mean', 'max'
        
        Returns:
            np.array: (768,) user profile embedding
        """
        # Parse history
        if isinstance(user_history_items[0], tuple):
            product_ids = [pid for pid, _ in user_history_items]
            weights = [w for _, w in user_history_items]
        else:
            product_ids = user_history_items
            weights = [1.0] * len(product_ids)
        
        # Get embeddings
        history_embeddings = []
        history_weights = []
        
        for pid, weight in zip(product_ids, weights):
            emb = self.get_embedding(pid)
            if emb is not None:
                history_embeddings.append(emb)
                history_weights.append(weight)
        
        if not history_embeddings:
            # Fallback: return mean embedding
            return np.zeros(self.embeddings.shape[1])
        
        history_embeddings = np.array(history_embeddings)
        history_weights = np.array(history_weights).reshape(-1, 1)
        
        # Aggregate
        if strategy == 'mean':
            profile = np.mean(history_embeddings, axis=0)
        elif strategy == 'weighted_mean':
            weights_norm = history_weights / history_weights.sum()
            profile = (history_embeddings * weights_norm).sum(axis=0)
        elif strategy == 'max':
            profile = np.max(history_embeddings, axis=0)
        else:
            raise ValueError(f"Unknown strategy: {strategy}")
        
        return profile
```

## Timeline Estimate

- **Loader + Recommender**: 2 days
- **Fallback logic**: 0.5 day
- **API endpoints**: 1 day
- **BERT integration + Reranking**: 2 days
- **Testing**: 1 day
- **Deployment setup**: 0.5 day
- **Total**: ~7 days

## Success Criteria

- [ ] Load model tá»« registry (<1 second)
- [ ] Generate recommendations (<100ms per user CF-only)
- [ ] Two-stage reranking (<200ms with BERT)
- [ ] BERT embeddings loaded and cached
- [ ] Cold-start fallback works
- [ ] API endpoints functional
- [ ] Hot-reload model without downtime
- [ ] Logging tracks latency, fallback rate, rerank metrics
- [ ] Docker deployment tested
